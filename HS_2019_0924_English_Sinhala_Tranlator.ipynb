{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SITHUM-GIT/Dictionary/blob/main/HS_2019_0924_English_Sinhala_Tranlator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwzDPiSDyDld"
      },
      "source": [
        "## **English to Sinhala Translation System using Transformer Neural Network**\n",
        "\n",
        "**COSC 44323 - Introduction To Deep Learning**\n",
        "\n",
        "**Mini Project No. 03**\n",
        "\n",
        "**Student Number: HS/2019/0924**\n",
        "\n",
        "**Name: P.G.S.N Ilangathilaka**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J4ny2ZO_bVq"
      },
      "source": [
        "**IMPORT LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7paPZcR6x_OP"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9CTLVsI_uWZ"
      },
      "source": [
        "**MOUNT DRIVE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PkA4-cMQyvfE",
        "outputId": "e376ffcd-5850-4e22-a73e-2b6ec4d5d078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAhjZ7Zm_yl4"
      },
      "source": [
        "**READ THE DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "hbdXgc60y6zY",
        "outputId": "89b0a291-8dd4-4c89-81da-9a9735345561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go.\tයන්න.\n",
            "Hi.\tආයුබෝවන්.\n",
            "Run.\tදුවන්න.\n",
            "Who?\tWHO?\n",
            "Wow!\tවාව්!\n",
            "Fire!\tගිනි!\n",
            "Help!\tඋදව්!\n",
            "Jump!\tපනින්න!\n",
            "Jump.\tපනින්න.\n",
            "Stop!\tනවත්වන්න!\n",
            "Wait!\tඉන්න!\n",
            "Go on.\tයන්න.\n",
            "Hello!\tආයුබෝවන්!\n",
            "Hurry!\tඉක්මන් කරන්න!\n",
            "I see.\tමම දකියි.\n",
            "I try.\tමම උත්සාහ කරනවා.\n",
            "I won!\tමම දිනුවා!\n",
            "Oh no!\tඅපොයි නෑ!\n",
            "Relax.\tසන්සුන් වන්න.\n",
            "Shoot!\tවෙඩි තියන්න!\n"
          ]
        }
      ],
      "source": [
        "text_file = \"/content/drive/MyDrive/Deep Learning/Dictionary/Sinhala to english Dataset 40.txt\"\n",
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "i = 0\n",
        "for line in lines:\n",
        "  print(line)\n",
        "  i = i + 1\n",
        "  if(i==20):\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "iNnVT3Qiy_qP",
        "outputId": "e14ecad1-e7a4-423c-ca3a-093a425913e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instead of laying off these workers, why don't we just cut their hours?\tමේ කම්කරුවන් දොට්ට දමනවා වෙනුවට අපි ඔවුන්ගේ පැය ගණන කපා නොගන්නේ මන්ද?\n",
            "The thieves pulled open all the drawers of the desk in search of money.\tහොරු සල්ලි හොයන්න මේසයේ ලාච්චු සේරම ඇරියා.\n",
            "Father kept in touch with us by mail and telephone while he was overseas.\tතාත්තා විදේශගතව සිටියදී තැපෑලෙන් සහ දුරකථනයෙන් අපිව සම්බන්ධ කරගත්තා.\n",
            "George Washington was the first president of the Unites States of America.\tජෝර්ජ් වොෂින්ටන් ඇමරිකා එක්සත් ජනපදයේ පළමු ජනාධිපති විය.\n",
            "Mother Teresa used the prize money for her work in India and around the world.\tතෙරේසා මවුතුමිය එම ත්‍යාග මුදල ඉන්දියාවේ සහ ලොව පුරා සිය කටයුතු සඳහා යෙදවූවාය.\n",
            "If you go to that supermarket, you can buy most things you use in your daily life.\tඔබ එම සුපිරි වෙළඳසැලට ගියහොත්, ඔබ එදිනෙදා ජීවිතයේ භාවිතා කරන බොහෝ දේ ඔබට මිලදී ගත හැකිය.\n",
            "The passengers who were injured in the accident were taken to the nearest hospital.\tඅනතුරින් තුවාල ලැබූ මගීන් ළඟම ඇති රෝහල වෙත රැගෙන ගොස් ඇත.\n",
            "Democracy is the worst form of government, except all the others that have been tried.\tප්‍රජාතන්ත්‍රවාදය යනු අත්හදා බලා ඇති අනෙක් සියල්ල හැර නරකම පාලන ක්‍රමයයි.\n",
            "If my boy had not been killed in the traffic accident, he would be a college student now.\tමගේ කොල්ලා රිය අනතුරකින් මිය නොගියේ නම්, ඔහු දැන් විශ්ව විද්‍යාල ශිෂ්‍යයෙකි.\n",
            "When I was a kid, touching bugs didn't bother me a bit. Now I can hardly stand looking at pictures of them.\tමම පොඩි කාලේ මකුණන් අල්ලන එක මට පොඩ්ඩක්වත් කරදරයක් වුණේ නැහැ. දැන් මට උන්ගෙ පින්තූර දිහා බලාගෙන ඉන්න අමාරුයි.\n"
          ]
        }
      ],
      "source": [
        "for x in range(len(lines)-10,len(lines)):\n",
        "  print(lines[x])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2xEU_wqAC16"
      },
      "source": [
        "**SPLIT BOTH TRANSLATION PAIRS OF ENGLISH AND SINHALA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "X1M4j5V-zEM4",
        "outputId": "1d358312-68a3-4781-80bf-d19518c31581"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('It took all night to climb Mt Fuji.', '[start] ෆුජි කන්ද තරණය කිරීමට මුළු රාත්\\u200dරිය ගත විය. [end]')\n",
            "('He decided to go abroad.', '[start] ඔහු විදේශගත වීමට තීරණය කළේය. [end]')\n",
            "('He missed the train by a minute.', '[start] ඔහුට විනාඩියකින් දුම්රිය මග හැරුණි. [end]')\n"
          ]
        }
      ],
      "source": [
        "text_pairs = []\n",
        "for line in lines:\n",
        "\n",
        "    if line.count(\"\\t\") == 1:\n",
        "        english, sinhala = line.split(\"\\t\", 1)\n",
        "        sinhala = \"[start] \" + sinhala.strip() + \" [end]\"\n",
        "        text_pairs.append((english.strip(), sinhala))\n",
        "    else:\n",
        "        print(\"Skipping line with incorrect format:\", line)\n",
        "\n",
        "for i in range(3):\n",
        "    print(random.choice(text_pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5aVRbpeALO6"
      },
      "source": [
        "**SHUFFLE THE DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "143xCHTZzWJe"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.shuffle(text_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Muw7nYqJAlbd"
      },
      "source": [
        "**SPITING THE DATASET INTO TRAINING, TESTING AND VALIDATION DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "OhE59G2UzZTE",
        "outputId": "27fe6ed6-58f0-4bfd-b8ca-8a9d9970076b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentences: 49202\n",
            "Training set size: 34442\n",
            "Validation set size: 7380\n",
            "Testing set size: 7380\n",
            "Total size of the dataset: 49202\n"
          ]
        }
      ],
      "source": [
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]\n",
        "print(\"Total sentences:\",len(text_pairs))\n",
        "print(\"Training set size:\",len(train_pairs))\n",
        "print(\"Validation set size:\",len(val_pairs))\n",
        "print(\"Testing set size:\",len(test_pairs))\n",
        "print(\"Total size of the dataset:\",len(train_pairs)+len(val_pairs)+len(test_pairs))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tZYKRh_Ayv1"
      },
      "source": [
        "**REMOVING PUNCTUATIONS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YT0-IkybzePp",
        "outputId": "fa748685-18b5-4400-ca2c-9083da4af0c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<=>\\\\?@\\\\\\\\\\\\^_`\\\\{\\\\|\\\\}\\\\~¿]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "f\"[{re.escape(strip_chars)}]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "G1IpzQoCzhML",
        "outputId": "c1ad448b-d682-4ac0-9d58-ae5d59f503e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'8'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "f\"{3+5}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW-cHLDSA9ge"
      },
      "source": [
        "**VECTORIZATION THE ENGLISH AND SINHALA TEXT PAIRS**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-sBCYVZlzkQ2"
      },
      "outputs": [],
      "source": [
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_sinhala_texts = [pair[1] for pair in train_pairs]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eIk3FECyzt0i"
      },
      "outputs": [],
      "source": [
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_sinhala_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgf3qth2BGct"
      },
      "source": [
        "**PREPARE THE DATASET FOR THE TRANSLATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jpsnEAf9z0rs",
        "outputId": "153070a3-0aaf-4501-8782-de39cfcb6e83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['english'].shape: (64, 20)\n",
            "inputs['sinhala'].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n",
            "({'english': array([[  17,    7,   28, ...,    0,    0,    0],\n",
            "       [   6, 1609,  363, ...,    0,    0,    0],\n",
            "       [1076,  900,  504, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [  16,    8,    2, ...,    0,    0,    0],\n",
            "       [  15,  470,   37, ...,    0,    0,    0],\n",
            "       [   7,   28,   40, ...,    0,    0,    0]]), 'sinhala': array([[   2,    5,   37, ...,    0,    0,    0],\n",
            "       [   2, 2282, 2629, ...,    0,    0,    0],\n",
            "       [   2, 5719, 2242, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [   2,  191,    6, ...,    0,    0,    0],\n",
            "       [   2,    9, 1232, ...,    0,    0,    0],\n",
            "       [   2,    5,   37, ...,    0,    0,    0]])}, array([[   5,   37,   41, ...,    0,    0,    0],\n",
            "       [2282, 2629,  188, ...,    0,    0,    0],\n",
            "       [5719, 2242,  273, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [ 191,    6,  111, ...,    0,    0,    0],\n",
            "       [   9, 1232,   53, ...,    0,    0,    0],\n",
            "       [   5,   37,   41, ...,    0,    0,    0]]))\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "def format_dataset(eng, sin):\n",
        "    eng = source_vectorization(eng)\n",
        "    sin = target_vectorization(sin)\n",
        "    return ({\n",
        "        \"english\": eng,\n",
        "        \"sinhala\": sin[:, :-1],\n",
        "    }, sin[:, 1:])\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, sin_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    sin_texts = list(sin_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, sin_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)\n",
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "    print(f\"inputs['sinhala'].shape: {inputs['sinhala'].shape}\")\n",
        "    print(f\"targets.shape: {targets.shape}\")\n",
        "inputs['english'].shape: (64, 20)\n",
        "inputs['sinhala'].shape: (64, 20)\n",
        "targets.shape: (64, 20)\n",
        "print(list(train_ds.as_numpy_iterator())[50])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKTN-IINBYUb"
      },
      "source": [
        "**TRANSFORMER ENCODER IMPLEMENTED AS A SUBCLASSED LAYER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0MSpMIu81gb6"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ao8PVw1XBnDM"
      },
      "source": [
        "**THE TRANSFORMER DECODER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "BjI4RLM_1nV_"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        else:\n",
        "            padding_mask = mask\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qXpWKlmBvot"
      },
      "source": [
        "**POSITION ENCODEING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Hu6R5TzC1uWa"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIoqbFF0B1DU"
      },
      "source": [
        "**END-TO-END TRANSFORMER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7xGNmAQU16yQ"
      },
      "outputs": [],
      "source": [
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 8\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"sinhala\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Xbwr2vUG1zVn",
        "outputId": "c64e8c06-207c-461a-d0bd-eda8c11c122c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " english (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " sinhala (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " positional_embedding (Posi  (None, None, 256)            3845120   ['english[0][0]']             \n",
            " tionalEmbedding)                                                                                 \n",
            "                                                                                                  \n",
            " positional_embedding_1 (Po  (None, None, 256)            3845120   ['sinhala[0][0]']             \n",
            " sitionalEmbedding)                                                                               \n",
            "                                                                                                  \n",
            " transformer_encoder (Trans  (None, None, 256)            3155456   ['positional_embedding[0][0]']\n",
            " formerEncoder)                                                                                   \n",
            "                                                                                                  \n",
            " transformer_decoder (Trans  (None, None, 256)            5259520   ['positional_embedding_1[0][0]\n",
            " formerDecoder)                                                     ',                            \n",
            "                                                                     'transformer_encoder[0][0]'] \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, None, 256)            0         ['transformer_decoder[0][0]'] \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, None, 15000)          3855000   ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 19960216 (76.14 MB)\n",
            "Trainable params: 19960216 (76.14 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "transformer.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfcgAdwWB9Ms"
      },
      "source": [
        "**TRAINING THE TRANSFORMER NEURAL NETWORK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "p35P05_G2G6O",
        "outputId": "b78f6f43-684a-4cae-a0a0-a2437a392f3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "539/539 [==============================] - 54s 83ms/step - loss: 5.1299 - accuracy: 0.3490 - val_loss: 4.1665 - val_accuracy: 0.4200\n",
            "Epoch 2/40\n",
            "539/539 [==============================] - 37s 68ms/step - loss: 4.0276 - accuracy: 0.4408 - val_loss: 3.5728 - val_accuracy: 0.4852\n",
            "Epoch 3/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 3.4986 - accuracy: 0.4929 - val_loss: 3.3067 - val_accuracy: 0.5177\n",
            "Epoch 4/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 3.1653 - accuracy: 0.5273 - val_loss: 3.2093 - val_accuracy: 0.5274\n",
            "Epoch 5/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 2.9239 - accuracy: 0.5562 - val_loss: 3.1315 - val_accuracy: 0.5394\n",
            "Epoch 6/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 2.7467 - accuracy: 0.5796 - val_loss: 3.0912 - val_accuracy: 0.5498\n",
            "Epoch 7/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 2.6166 - accuracy: 0.5985 - val_loss: 3.0699 - val_accuracy: 0.5520\n",
            "Epoch 8/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 2.5172 - accuracy: 0.6151 - val_loss: 3.0521 - val_accuracy: 0.5624\n",
            "Epoch 9/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 2.4418 - accuracy: 0.6277 - val_loss: 3.0340 - val_accuracy: 0.5672\n",
            "Epoch 10/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 2.3822 - accuracy: 0.6399 - val_loss: 3.0825 - val_accuracy: 0.5619\n",
            "Epoch 11/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 2.3366 - accuracy: 0.6495 - val_loss: 3.0448 - val_accuracy: 0.5698\n",
            "Epoch 12/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 2.2960 - accuracy: 0.6585 - val_loss: 3.0703 - val_accuracy: 0.5733\n",
            "Epoch 13/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 2.2624 - accuracy: 0.6663 - val_loss: 3.1084 - val_accuracy: 0.5715\n",
            "Epoch 14/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 2.2314 - accuracy: 0.6728 - val_loss: 3.0983 - val_accuracy: 0.5743\n",
            "Epoch 15/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 2.2006 - accuracy: 0.6800 - val_loss: 3.1286 - val_accuracy: 0.5743\n",
            "Epoch 16/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 2.1711 - accuracy: 0.6866 - val_loss: 3.1212 - val_accuracy: 0.5785\n",
            "Epoch 17/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 2.1458 - accuracy: 0.6919 - val_loss: 3.2094 - val_accuracy: 0.5726\n",
            "Epoch 18/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 2.1198 - accuracy: 0.6974 - val_loss: 3.1739 - val_accuracy: 0.5775\n",
            "Epoch 19/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 2.0984 - accuracy: 0.7021 - val_loss: 3.2217 - val_accuracy: 0.5777\n",
            "Epoch 20/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 2.0798 - accuracy: 0.7066 - val_loss: 3.1978 - val_accuracy: 0.5759\n",
            "Epoch 21/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 2.0527 - accuracy: 0.7119 - val_loss: 3.2317 - val_accuracy: 0.5760\n",
            "Epoch 22/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 2.0362 - accuracy: 0.7152 - val_loss: 3.2318 - val_accuracy: 0.5778\n",
            "Epoch 23/40\n",
            "539/539 [==============================] - 38s 71ms/step - loss: 2.0189 - accuracy: 0.7193 - val_loss: 3.2588 - val_accuracy: 0.5784\n",
            "Epoch 24/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 1.9965 - accuracy: 0.7231 - val_loss: 3.2845 - val_accuracy: 0.5782\n",
            "Epoch 25/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 1.9851 - accuracy: 0.7255 - val_loss: 3.2711 - val_accuracy: 0.5789\n",
            "Epoch 26/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 1.9686 - accuracy: 0.7284 - val_loss: 3.2848 - val_accuracy: 0.5820\n",
            "Epoch 27/40\n",
            "539/539 [==============================] - 37s 68ms/step - loss: 1.9572 - accuracy: 0.7316 - val_loss: 3.3434 - val_accuracy: 0.5786\n",
            "Epoch 28/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 1.9380 - accuracy: 0.7352 - val_loss: 3.3434 - val_accuracy: 0.5768\n",
            "Epoch 29/40\n",
            "539/539 [==============================] - 37s 68ms/step - loss: 1.9306 - accuracy: 0.7370 - val_loss: 3.3263 - val_accuracy: 0.5827\n",
            "Epoch 30/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 1.9132 - accuracy: 0.7400 - val_loss: 3.3375 - val_accuracy: 0.5842\n",
            "Epoch 31/40\n",
            "539/539 [==============================] - 38s 71ms/step - loss: 1.9055 - accuracy: 0.7410 - val_loss: 3.4050 - val_accuracy: 0.5774\n",
            "Epoch 32/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 1.8923 - accuracy: 0.7442 - val_loss: 3.4175 - val_accuracy: 0.5796\n",
            "Epoch 33/40\n",
            "539/539 [==============================] - 38s 71ms/step - loss: 1.8812 - accuracy: 0.7464 - val_loss: 3.4073 - val_accuracy: 0.5783\n",
            "Epoch 34/40\n",
            "539/539 [==============================] - 38s 71ms/step - loss: 1.8708 - accuracy: 0.7487 - val_loss: 3.4364 - val_accuracy: 0.5794\n",
            "Epoch 35/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 1.8578 - accuracy: 0.7503 - val_loss: 3.4927 - val_accuracy: 0.5787\n",
            "Epoch 36/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 1.8544 - accuracy: 0.7511 - val_loss: 3.4447 - val_accuracy: 0.5836\n",
            "Epoch 37/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 1.8378 - accuracy: 0.7538 - val_loss: 3.4942 - val_accuracy: 0.5782\n",
            "Epoch 38/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 1.8251 - accuracy: 0.7564 - val_loss: 3.4703 - val_accuracy: 0.5800\n",
            "Epoch 39/40\n",
            "539/539 [==============================] - 37s 69ms/step - loss: 1.8183 - accuracy: 0.7571 - val_loss: 3.4951 - val_accuracy: 0.5823\n",
            "Epoch 40/40\n",
            "539/539 [==============================] - 38s 71ms/step - loss: 1.8067 - accuracy: 0.7594 - val_loss: 3.5211 - val_accuracy: 0.5816\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78a7b07fbcd0>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "transformer.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "transformer.fit(train_ds, epochs=40, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZzW_ydnIGlo"
      },
      "source": [
        "**MANUAL TESTING OF THE TRANSLATION MODEL WITH 10 NEW SENTENCES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "xuE2VEb32sPa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2e0e5861-0382-4735-e67a-9c530c89ae04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "Trains are running on schedule.\n",
            "[start] දුම්රිය නියමිත වේලාවට පැමිණේ [end]\n",
            "-\n",
            "My sister became a college student.\n",
            "[start] මගේ සහෝදරිය හොඳ ශිෂ්‍යයෙකි [end]\n",
            "-\n",
            "Where did you learn that?\n",
            "[start] ඔබ එය මිලදී ගත්තේ කොහෙන්ද [end]\n",
            "-\n",
            "An accident just happened.\n",
            "[start] අනතුරක් වූ අතර පමණි [end]\n",
            "-\n",
            "I would like to be alone.\n",
            "[start] මම තනියම ඉන්න කැමතියි [end]\n",
            "-\n",
            "I told Tom that I would do my best.\n",
            "[start] මම ටොම්ට කිව්වා මම කලින් ඒක කරන්න යනවා කියලා [end]\n",
            "-\n",
            "She didn't know what to say to him.\n",
            "[start] ඔහුව දැන සිටියේ කුමක්දැයි ඇයට කිසිවක් කීවේ නැත [end]\n",
            "-\n",
            "Do you think you can live on a dollar a day in America?\n",
            "[start] ඔබට අවම වශයෙන් ලස්සන ජීවිතයක් ගත හැකිද [end]\n",
            "-\n",
            "If only I had known the answer yesterday!\n",
            "[start] එය ප්‍රංශ භාෂාව පිළිබඳ එකම භාෂාවෙන් පිළිතුරු පමණි [end]\n",
            "-\n",
            "It's pouring.\n",
            "[start] එය අවුල් සහගත ය [end]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "sin_vocab = target_vectorization.get_vocabulary()\n",
        "sin_index_lookup = dict(zip(range(len(sin_vocab)), sin_vocab))\n",
        "max_decoded_sentence_length = 10\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization(\n",
        "            [decoded_sentence])[:, :-1]\n",
        "        predictions = transformer(\n",
        "            [tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = sin_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(10):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}